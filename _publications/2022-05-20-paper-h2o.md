<!-- ---
title: "When to Trust Your Simulator: Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning"
collection: publications
# permalink: /publication/2009-10-01-paper-title-number-1
# excerpt: 'Is it possible to combine learning from limited real data in offline RL and unrestricted
# exploration through imperfect simulators in online RL to address the drawbacks of
# both approaches? In this study, we propose the Dynamics-Aware Hybrid Offline-
#  and-Online Reinforcement Learning (H2O) framework to provide an affirmative
#  answer to this question. H2O introduces a dynamics-aware policy evaluation
#  scheme, which adaptively penalizes the Q function learning on simulated state-
#  action pairs with large dynamics gaps, while also simultaneously allowing learning
#  from a fixed real-world dataset.'
# date: 2009-10-01
# venue: 'Haoyi Niu, Shubham Sharma, Yiwen Qiu, Ming Li, Guyue Zhou, Jianming Hu, Xianyuan Zhan'
paperurl: 'https://arxiv.org/abs/2206.13464v1'
---
Haoyi Niu, Shubham Sharma, **Yiwen Qiu**, Ming Li, Guyue Zhou, Jianming Hu, Xianyuan Zhan
* Abstract: Is it possible to combine learning from limited real data in offline RL and unrestricted exploration through imperfect simulators in online RL to address the drawbacks of both approaches? In this study, we propose the Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning (H2O) framework to provide an affirmative
 answer to this question. H2O introduces a dynamics-aware policy evaluation
 scheme, which adaptively penalizes the Q function learning on simulated state-
 action pairs with large dynamics gaps, while also simultaneously allowing learning
 from a fixed real-world dataset.
<!-- # exploration through imperfect simulators in online RL to address the drawbacks of
# both approaches? In this study, we propose the Dynamics-Aware Hybrid Offline-
#  and-Online Reinforcement Learning (H2O) framework to provide an affirmative
#  answer to this question. H2O introduces a dynamics-aware policy evaluation
#  scheme, which adaptively penalizes the Q function learning on simulated state-
#  action pairs with large dynamics gaps, while also simultaneously allowing learning
#  from a fixed real-world dataset. -->

